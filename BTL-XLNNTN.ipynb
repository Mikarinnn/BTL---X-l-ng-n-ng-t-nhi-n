{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55f479e",
   "metadata": {},
   "source": [
    "1. Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76b8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc dữ liệu song ngữ\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\data\\train.en', 'r', encoding='utf-8') as f:\n",
    "    en_sentences = f.read().strip().split('\\n')\n",
    "\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\data\\train.vi', 'r', encoding='utf-8') as f:\n",
    "    vi_sentences = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d738244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#làm sạch\n",
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.lower().strip()               #lower chuyển tất cả các chữ thành chữ thường, strip xóa khoảng trắng đầu và cuối câu\n",
    "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s) #thêm dấu cách trước và sau các dấu câu\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)       #loại bỏ các khoảng trắng thừa\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,’'`àáảãạăằắẳẵặâầấẩẫậèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđ]\", \" \", s) #loại bỏ các ký tự không cần thiết\n",
    "    s = s.strip() \n",
    "    return s\n",
    "\n",
    "#làm sạch toàn bộ các câu song ngữ trước khi đưa vào mô hình\n",
    "en_sentences = [clean_sentence(s) for s in en_sentences]\n",
    "vi_sentences = [clean_sentence(s) for s in vi_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "938d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_sentences = ['<start> ' + s + ' <end>' for s in vi_sentences] #thêm token start và end cho các câu tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eadd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo token cho các câu, chuyển các câu thành token ID\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Tiếng Anh\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(en_sentences)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_sentences)\n",
    "\n",
    "#Tiếng Việt\n",
    "vi_tokenizer = Tokenizer(filters='')\n",
    "vi_tokenizer.fit_on_texts(vi_sentences)\n",
    "vi_sequences = vi_tokenizer.texts_to_sequences(vi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8014ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#độ dài trong danh sách tiếng anh và tiếng việt\n",
    "MAX_LEN = 80\n",
    "\n",
    "en_padded = pad_sequences(en_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "#áp dụng padding cho tất cả chuỗi senquences\n",
    "#Chỉ dùng 50.000 câu để thử\n",
    "en_sequences_small = en_sequences[:50000]\n",
    "vi_sequences_small = vi_sequences[:50000]\n",
    "\n",
    "en_padded = pad_sequences(en_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87674878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia ra hai phần train và valid (đánh giá) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "en_train, en_val, vi_train, vi_val = train_test_split(en_padded, vi_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43fe91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo data set\n",
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(en_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((en_val, vi_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f2f2",
   "metadata": {},
   "source": [
    "2. Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96eb80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Định nghĩa mô hình Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91ad1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo mô hình\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1   \n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f2318",
   "metadata": {},
   "source": [
    "3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c899156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Định nghĩa optimizer và loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90261354",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(en_padded)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_padded, vi_padded))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "701c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tf.constant(vi_tokenizer.word_index['<start>'], dtype=tf.int32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # Tạo dec_input bằng Tensor constant\n",
    "        dec_input = tf.expand_dims(tf.repeat(start_token, BATCH_SIZE), 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / tf.cast(targ.shape[1], tf.float32)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa7be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.9089\n",
      "Epoch 2 Loss 1.7347\n",
      "Epoch 3 Loss 1.5950\n",
      "Epoch 4 Loss 1.5008\n",
      "Epoch 5 Loss 1.4321\n",
      "Epoch 6 Loss 1.3772\n",
      "Epoch 7 Loss 1.3287\n",
      "Epoch 8 Loss 1.2841\n",
      "Epoch 9 Loss 1.2390\n",
      "Epoch 10 Loss 1.1915\n",
      "Epoch 11 Loss 1.1464\n",
      "Epoch 12 Loss 1.1144\n",
      "Epoch 13 Loss 1.0755\n",
      "Epoch 14 Loss 1.0343\n",
      "Epoch 15 Loss 0.9985\n",
      "Epoch 16 Loss 0.9725\n",
      "Epoch 17 Loss 0.9490\n",
      "Epoch 18 Loss 0.9574\n",
      "Epoch 19 Loss 0.9082\n",
      "Epoch 20 Loss 0.9015\n",
      "Epoch 21 Loss 0.8745\n",
      "Epoch 22 Loss 0.8648\n",
      "Epoch 23 Loss 0.8324\n",
      "Epoch 24 Loss 0.8165\n",
      "Epoch 25 Loss 0.8208\n",
      "Epoch 26 Loss 0.7891\n",
      "Epoch 27 Loss 0.7729\n",
      "Epoch 28 Loss 0.7793\n",
      "Epoch 29 Loss 0.7896\n",
      "Epoch 30 Loss 0.7542\n",
      "Epoch 31 Loss 0.7417\n",
      "Epoch 32 Loss 0.7192\n",
      "Epoch 33 Loss 0.7187\n",
      "Epoch 34 Loss 0.7947\n",
      "Epoch 35 Loss 0.7876\n",
      "Epoch 36 Loss 0.7059\n",
      "Epoch 37 Loss 0.7001\n",
      "Epoch 38 Loss 0.6871\n",
      "Epoch 39 Loss 0.7253\n",
      "Epoch 40 Loss 0.6843\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):                                                 #Bắt đầu vòng lặp\n",
    "    enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    \n",
    "# Lưu mô hình mỗi epoch\n",
    "encoder.save_weights(f'encoder_epoch{epoch+1}.weights.h5')\n",
    "decoder.save_weights(f'decoder_epoch{epoch+1}.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ab01a",
   "metadata": {},
   "source": [
    "4. Đánh giá huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54add12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đánh giá mô hình trên tập validation...\n",
      "\n",
      "[1] Input (EN):     they  apos re actually about improving houses .\n",
      "    Target (VI):    <start> mà là cải thiện nhà cửa . <end>\n",
      "    Predicted (VI): chúng thực sự là sự tiến hành rất trong gia .\n",
      "    BLEU score:     0.0257\n",
      "------------------------------------------------------------\n",
      "[2] Input (EN):     we start on day one of every project    we  apos ve learned , we don  apos t make promises , we don  apos t do reports .\n",
      "    Target (VI):    <start> mỗi ngày chúng tôi làm   dự án   theo kinh nghiệm , chúng tôi không hứa , không báo cáo . <end>\n",
      "    Predicted (VI): chúng tôi bắt đầu ngày một nhóm nghiên cứu mà chúng tôi đã học , chúng ta không làm là người tự nhiên , chúng ta không làm cho đến .\n",
      "    BLEU score:     0.0401\n",
      "------------------------------------------------------------\n",
      "[3] Input (EN):     we arrive in the morning with tools , tons of equipment , trades , and we train up a local team on the first day to start work .\n",
      "    Target (VI):    <start> mỗi sáng chúng tôi mang dụng cụ đến , hàng tấn thiết bị , giao dịch , và đào tạo các nhóm bản địa để làm việc ngay ngày đầu . <end>\n",
      "    Predicted (VI): chúng tôi đến giờ sáng tạo ra , và các thiết bị , công việc , công việc , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc của chúng , công việc của chúng , công việc , công việc\n",
      "    BLEU score:     0.0029\n",
      "------------------------------------------------------------\n",
      "[4] Input (EN):     by the evening of the first day , a few houses in that community are better than when we started in the morning .\n",
      "    Target (VI):    <start> tối ngày đầu tiên , một số nhà trong cộng đồng đó đã tốt hơn so với lúc sáng . <end>\n",
      "    Predicted (VI): bằng một buổi tối , ngày , một vài nhà những nhóm ở trong cộng đồng tốt hơn khi chúng tôi bắt đầu vào buổi sáng .\n",
      "    BLEU score:     0.0844\n",
      "------------------------------------------------------------\n",
      "[5] Input (EN):     that work continues for six to    months until all the houses are improved and we  apos ve spent our budget of   ,     dollars total per house .\n",
      "    Target (VI):    <start> công việc tiếp tục triển khai   đến    tháng đến khi mọi nhà đều được sửa với chi phí trung bình   .     đô la cho mỗi nhà . <end>\n",
      "    Predicted (VI): điều này đã tiếp tục thay vì đã đến tháng tới cho đến cho đến khi những ngôi nhà đang tiến và bắt đầu và chúng ta đã dành toàn bộ ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của chúng tôi , chúng ta đã dành cả ngân hàng của\n",
      "    BLEU score:     0.0018\n",
      "------------------------------------------------------------\n",
      "[6] Input (EN):     that  apos s our average budget .\n",
      "    Target (VI):    <start> đó là ngân sách trung bình . <end>\n",
      "    Predicted (VI): ngân sách trung tâm của chúng ta .\n",
      "    BLEU score:     0.1565\n",
      "------------------------------------------------------------\n",
      "[7] Input (EN):     at the end of six months to a year , we test every house again .\n",
      "    Target (VI):    <start> hết   tháng đến   năm , chúng tôi kiểm tra lại . <end>\n",
      "    Predicted (VI): cuối tháng năm , chúng tôi kiểm tra mỗi nhà học hỏi .\n",
      "    BLEU score:     0.4090\n",
      "------------------------------------------------------------\n",
      "[8] Input (EN):     it  apos s very easy to spend money .\n",
      "    Target (VI):    <start> tiêu tiền thì rất dễ . nhưng rất khó để sửa <end>\n",
      "    Predicted (VI): nó rất dễ dàng tiền .\n",
      "    BLEU score:     0.0360\n",
      "------------------------------------------------------------\n",
      "[9] Input (EN):     it  apos s very difficult to improve the function of all those parts of the house , and for a whole house , the nine healthy living practices , we test , check and fix     items in every house .\n",
      "    Target (VI):    <start> chức năng toàn bộ vật dụng trong nhà . so toàn bộ nhà , cùng   thực hành sống khoẻ mạnh , chúng tôi kiểm nghiệm và sửa     đồ vật trong mỗi nhà . <end>\n",
      "    Predicted (VI): rất khó để cải thiện chức năng của các phần của các nhà này , nhà của nhà , và cho cả nhà , ngôi nhà , và một ngôi nhà , và cả các nhà , một cuộc ngôi nhà , và công việc , chúng tôi kiểm tra , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm soát , và kiểm\n",
      "    BLEU score:     0.0039\n",
      "------------------------------------------------------------\n",
      "[10] Input (EN):     and these are the results we can get with our   ,     dollars .\n",
      "    Target (VI):    <start> và đây là kết quả chúng tôi có từ   .     đô la . <end>\n",
      "    Predicted (VI): và đây là những kết quả mà chúng ta có thể làm với chúng ta ,\n",
      "    BLEU score:     0.1074\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎯 Điểm BLEU trung bình trên tập validation: 0.0689\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "#Định nghĩa lại mô hình Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "#Khởi tạo lại mô hình\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "# Tạo một input giả để build encoder/decoder\n",
    "sample_input = tf.random.uniform((1, 10), dtype=tf.int32, minval=0, maxval=vocab_inp_size)\n",
    "\n",
    "# Build encoder\n",
    "encoder.initialize_hidden_state(1)  # Gọi hidden\n",
    "_ = encoder(sample_input, encoder.initialize_hidden_state(1))\n",
    "\n",
    "# Build decoder\n",
    "sample_output = tf.random.uniform((1, 1), dtype=tf.int32, minval=0, maxval=vocab_tar_size)\n",
    "_ = decoder(sample_output, encoder.initialize_hidden_state(1), tf.random.uniform((1, 10, units)))\n",
    "\n",
    "\n",
    "\n",
    "#load trọng số\n",
    "encoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\encoder_epoch40.weights.h5')\n",
    "decoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\decoder_epoch40.weights.h5')\n",
    "\n",
    "# Tính độ dài tối đa cho đầu vào và đầu ra (nếu chưa có)\n",
    "max_length_inp = max(len(seq) for seq in en_tokenizer.texts_to_sequences(en_sentences))\n",
    "max_length_targ = max(len(seq) for seq in vi_tokenizer.texts_to_sequences(vi_sentences))\n",
    "\n",
    "\n",
    "# Lấy tập validation từ 10% cuối\n",
    "val_input_sentences = en_sentences[-100:]\n",
    "val_target_sentences = vi_sentences[-100:]\n",
    "\n",
    "# Tiền xử lý 1 câu\n",
    "def preprocess_sentence(sentence):\n",
    "    import numpy as np\n",
    "    if isinstance(sentence, np.ndarray):\n",
    "        if sentence.ndim == 0:\n",
    "            sentence = sentence.item()\n",
    "        else:\n",
    "            sentence = sentence[0]\n",
    "    elif isinstance(sentence, list):\n",
    "        if isinstance(sentence[0], str):\n",
    "            sentence = sentence[0]\n",
    "        else:\n",
    "            sentence = ' '.join(str(w) for w in sentence)\n",
    "    return sentence.lower().strip()\n",
    "\n",
    "# Hàm dịch một câu\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# Tính BLEU cho 1 cặp câu\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "# Đánh giá trên tập validation\n",
    "bleu_scores = []\n",
    "\n",
    "print(\"Đang đánh giá mô hình trên tập validation...\\n\")\n",
    "for i in range(len(val_input_sentences)):\n",
    "    input_sentence = val_input_sentences[i]\n",
    "    target_sentence = val_target_sentences[i]\n",
    "\n",
    "    pred_sentence = evaluate(input_sentence)\n",
    "\n",
    "    ref = preprocess_sentence(target_sentence).split()\n",
    "    hyp = pred_sentence.split()\n",
    "\n",
    "    score = compute_bleu(ref, hyp)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "    # In 10 câu đầu để kiểm tra\n",
    "    if i < 10:\n",
    "        print(f\"[{i+1}] Input (EN):     {input_sentence}\")\n",
    "        print(f\"    Target (VI):    {target_sentence}\")\n",
    "        print(f\"    Predicted (VI): {pred_sentence}\")\n",
    "        print(f\"    BLEU score:     {score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# BLEU trung bình\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"\\n Điểm BLEU trung bình trên tập validation: {average_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc964ef",
   "metadata": {},
   "source": [
    "5. Demo web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2628d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:26] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:40] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:47] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:56] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:28:02] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from flask import Flask, request, render_template_string\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Hàm dịch một câu\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Demo Dịch Máy Anh-Việt</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background: #f4f6f8;\n",
    "            margin: 0;\n",
    "            padding: 40px;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            align-items: center;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        form {\n",
    "            background: #fff;\n",
    "            padding: 20px 30px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "            width: 100%;\n",
    "            max-width: 600px;\n",
    "        }\n",
    "        textarea {\n",
    "            width: 100%;\n",
    "            height: 100px;\n",
    "            padding: 10px;\n",
    "            font-size: 16px;\n",
    "            resize: none;\n",
    "            border: 1px solid #ccc;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        button {\n",
    "            margin-top: 10px;\n",
    "            padding: 10px 20px;\n",
    "            font-size: 16px;\n",
    "            background: #007bff;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        button:hover {\n",
    "            background: #0056b3;\n",
    "        }\n",
    "        .result {\n",
    "            margin-top: 30px;\n",
    "            background: #e8f0fe;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            max-width: 600px;\n",
    "            width: 100%;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>🌍 Dịch Máy Anh-Việt</h1>\n",
    "    <form method=\"POST\">\n",
    "        <textarea name=\"input_text\" placeholder=\"Nhập câu tiếng Anh...\" required></textarea><br/>\n",
    "        <button type=\"submit\">Dịch</button>\n",
    "    </form>\n",
    "    {% if translation %}\n",
    "    <div class=\"result\">\n",
    "        <h2>Kết quả dịch:</h2>\n",
    "        <p>{{ translation }}</p>\n",
    "    </div>\n",
    "    {% endif %}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    translation = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        input_text = request.form[\"input_text\"]\n",
    "        translation = evaluate(input_text)\n",
    "    return render_template_string(html, translation=translation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
