{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55f479e",
   "metadata": {},
   "source": [
    "1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76b8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªçc d·ªØ li·ªáu song ng·ªØ\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\data\\train.en', 'r', encoding='utf-8') as f:\n",
    "    en_sentences = f.read().strip().split('\\n')\n",
    "\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\data\\train.vi', 'r', encoding='utf-8') as f:\n",
    "    vi_sentences = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d738244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l√†m s·∫°ch\n",
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.lower().strip()               #lower chuy·ªÉn t·∫•t c·∫£ c√°c ch·ªØ th√†nh ch·ªØ th∆∞·ªùng, strip x√≥a kho·∫£ng tr·∫Øng ƒë·∫ßu v√† cu·ªëi c√¢u\n",
    "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s) #th√™m d·∫•u c√°ch tr∆∞·ªõc v√† sau c√°c d·∫•u c√¢u\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)       #lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng th·ª´a\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,‚Äô'`√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]\", \" \", s) #lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt\n",
    "    s = s.strip() \n",
    "    return s\n",
    "\n",
    "#l√†m s·∫°ch to√†n b·ªô c√°c c√¢u song ng·ªØ tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh\n",
    "en_sentences = [clean_sentence(s) for s in en_sentences]\n",
    "vi_sentences = [clean_sentence(s) for s in vi_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "938d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_sentences = ['<start> ' + s + ' <end>' for s in vi_sentences] #th√™m token start v√† end cho c√°c c√¢u ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eadd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o token cho c√°c c√¢u, chuy·ªÉn c√°c c√¢u th√†nh token ID\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Ti·∫øng Anh\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(en_sentences)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_sentences)\n",
    "\n",
    "#Ti·∫øng Vi·ªát\n",
    "vi_tokenizer = Tokenizer(filters='')\n",
    "vi_tokenizer.fit_on_texts(vi_sentences)\n",
    "vi_sequences = vi_tokenizer.texts_to_sequences(vi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8014ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#ƒë·ªô d√†i trong danh s√°ch ti·∫øng anh v√† ti·∫øng vi·ªát\n",
    "MAX_LEN = 80\n",
    "\n",
    "en_padded = pad_sequences(en_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "#√°p d·ª•ng padding cho t·∫•t c·∫£ chu·ªói senquences\n",
    "#Ch·ªâ d√πng 50.000 c√¢u ƒë·ªÉ th·ª≠\n",
    "en_sequences_small = en_sequences[:50000]\n",
    "vi_sequences_small = vi_sequences[:50000]\n",
    "\n",
    "en_padded = pad_sequences(en_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87674878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia ra hai ph·∫ßn train v√† valid (ƒë√°nh gi√°) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "en_train, en_val, vi_train, vi_val = train_test_split(en_padded, vi_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43fe91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o data set\n",
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(en_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((en_val, vi_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f2f2",
   "metadata": {},
   "source": [
    "2. X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96eb80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªãnh nghƒ©a m√¥ h√¨nh Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91ad1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1   \n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f2318",
   "metadata": {},
   "source": [
    "3. Hu·∫•n luy·ªán m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c899156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªãnh nghƒ©a optimizer v√† loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90261354",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(en_padded)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_padded, vi_padded))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "701c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tf.constant(vi_tokenizer.word_index['<start>'], dtype=tf.int32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # T·∫°o dec_input b·∫±ng Tensor constant\n",
    "        dec_input = tf.expand_dims(tf.repeat(start_token, BATCH_SIZE), 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / tf.cast(targ.shape[1], tf.float32)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa7be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.9089\n",
      "Epoch 2 Loss 1.7347\n",
      "Epoch 3 Loss 1.5950\n",
      "Epoch 4 Loss 1.5008\n",
      "Epoch 5 Loss 1.4321\n",
      "Epoch 6 Loss 1.3772\n",
      "Epoch 7 Loss 1.3287\n",
      "Epoch 8 Loss 1.2841\n",
      "Epoch 9 Loss 1.2390\n",
      "Epoch 10 Loss 1.1915\n",
      "Epoch 11 Loss 1.1464\n",
      "Epoch 12 Loss 1.1144\n",
      "Epoch 13 Loss 1.0755\n",
      "Epoch 14 Loss 1.0343\n",
      "Epoch 15 Loss 0.9985\n",
      "Epoch 16 Loss 0.9725\n",
      "Epoch 17 Loss 0.9490\n",
      "Epoch 18 Loss 0.9574\n",
      "Epoch 19 Loss 0.9082\n",
      "Epoch 20 Loss 0.9015\n",
      "Epoch 21 Loss 0.8745\n",
      "Epoch 22 Loss 0.8648\n",
      "Epoch 23 Loss 0.8324\n",
      "Epoch 24 Loss 0.8165\n",
      "Epoch 25 Loss 0.8208\n",
      "Epoch 26 Loss 0.7891\n",
      "Epoch 27 Loss 0.7729\n",
      "Epoch 28 Loss 0.7793\n",
      "Epoch 29 Loss 0.7896\n",
      "Epoch 30 Loss 0.7542\n",
      "Epoch 31 Loss 0.7417\n",
      "Epoch 32 Loss 0.7192\n",
      "Epoch 33 Loss 0.7187\n",
      "Epoch 34 Loss 0.7947\n",
      "Epoch 35 Loss 0.7876\n",
      "Epoch 36 Loss 0.7059\n",
      "Epoch 37 Loss 0.7001\n",
      "Epoch 38 Loss 0.6871\n",
      "Epoch 39 Loss 0.7253\n",
      "Epoch 40 Loss 0.6843\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):                                                 #B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p\n",
    "    enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    \n",
    "# L∆∞u m√¥ h√¨nh m·ªói epoch\n",
    "encoder.save_weights(f'encoder_epoch{epoch+1}.weights.h5')\n",
    "decoder.save_weights(f'decoder_epoch{epoch+1}.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ab01a",
   "metadata": {},
   "source": [
    "4. ƒê√°nh gi√° hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54add12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\n",
      "\n",
      "[1] Input (EN):     they  apos re actually about improving houses .\n",
      "    Target (VI):    <start> m√† l√† c·∫£i thi·ªán nh√† c·ª≠a . <end>\n",
      "    Predicted (VI): ch√∫ng th·ª±c s·ª± l√† s·ª± ti·∫øn h√†nh r·∫•t trong gia .\n",
      "    BLEU score:     0.0257\n",
      "------------------------------------------------------------\n",
      "[2] Input (EN):     we start on day one of every project    we  apos ve learned , we don  apos t make promises , we don  apos t do reports .\n",
      "    Target (VI):    <start> m·ªói ng√†y ch√∫ng t√¥i l√†m   d·ª± √°n   theo kinh nghi·ªám , ch√∫ng t√¥i kh√¥ng h·ª©a , kh√¥ng b√°o c√°o . <end>\n",
      "    Predicted (VI): ch√∫ng t√¥i b·∫Øt ƒë·∫ßu ng√†y m·ªôt nh√≥m nghi√™n c·ª©u m√† ch√∫ng t√¥i ƒë√£ h·ªçc , ch√∫ng ta kh√¥ng l√†m l√† ng∆∞·ªùi t·ª± nhi√™n , ch√∫ng ta kh√¥ng l√†m cho ƒë·∫øn .\n",
      "    BLEU score:     0.0401\n",
      "------------------------------------------------------------\n",
      "[3] Input (EN):     we arrive in the morning with tools , tons of equipment , trades , and we train up a local team on the first day to start work .\n",
      "    Target (VI):    <start> m·ªói s√°ng ch√∫ng t√¥i mang d·ª•ng c·ª• ƒë·∫øn , h√†ng t·∫•n thi·∫øt b·ªã , giao d·ªãch , v√† ƒë√†o t·∫°o c√°c nh√≥m b·∫£n ƒë·ªãa ƒë·ªÉ l√†m vi·ªác ngay ng√†y ƒë·∫ßu . <end>\n",
      "    Predicted (VI): ch√∫ng t√¥i ƒë·∫øn gi·ªù s√°ng t·∫°o ra , v√† c√°c thi·∫øt b·ªã , c√¥ng vi·ªác , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác c·ªßa ch√∫ng , c√¥ng vi·ªác , c√¥ng vi·ªác\n",
      "    BLEU score:     0.0029\n",
      "------------------------------------------------------------\n",
      "[4] Input (EN):     by the evening of the first day , a few houses in that community are better than when we started in the morning .\n",
      "    Target (VI):    <start> t·ªëi ng√†y ƒë·∫ßu ti√™n , m·ªôt s·ªë nh√† trong c·ªông ƒë·ªìng ƒë√≥ ƒë√£ t·ªët h∆°n so v·ªõi l√∫c s√°ng . <end>\n",
      "    Predicted (VI): b·∫±ng m·ªôt bu·ªïi t·ªëi , ng√†y , m·ªôt v√†i nh√† nh·ªØng nh√≥m ·ªü trong c·ªông ƒë·ªìng t·ªët h∆°n khi ch√∫ng t√¥i b·∫Øt ƒë·∫ßu v√†o bu·ªïi s√°ng .\n",
      "    BLEU score:     0.0844\n",
      "------------------------------------------------------------\n",
      "[5] Input (EN):     that work continues for six to    months until all the houses are improved and we  apos ve spent our budget of   ,     dollars total per house .\n",
      "    Target (VI):    <start> c√¥ng vi·ªác ti·∫øp t·ª•c tri·ªÉn khai   ƒë·∫øn    th√°ng ƒë·∫øn khi m·ªçi nh√† ƒë·ªÅu ƒë∆∞·ª£c s·ª≠a v·ªõi chi ph√≠ trung b√¨nh   .     ƒë√¥ la cho m·ªói nh√† . <end>\n",
      "    Predicted (VI): ƒëi·ªÅu n√†y ƒë√£ ti·∫øp t·ª•c thay v√¨ ƒë√£ ƒë·∫øn th√°ng t·ªõi cho ƒë·∫øn cho ƒë·∫øn khi nh·ªØng ng√¥i nh√† ƒëang ti·∫øn v√† b·∫Øt ƒë·∫ßu v√† ch√∫ng ta ƒë√£ d√†nh to√†n b·ªô ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa ch√∫ng t√¥i , ch√∫ng ta ƒë√£ d√†nh c·∫£ ng√¢n h√†ng c·ªßa\n",
      "    BLEU score:     0.0018\n",
      "------------------------------------------------------------\n",
      "[6] Input (EN):     that  apos s our average budget .\n",
      "    Target (VI):    <start> ƒë√≥ l√† ng√¢n s√°ch trung b√¨nh . <end>\n",
      "    Predicted (VI): ng√¢n s√°ch trung t√¢m c·ªßa ch√∫ng ta .\n",
      "    BLEU score:     0.1565\n",
      "------------------------------------------------------------\n",
      "[7] Input (EN):     at the end of six months to a year , we test every house again .\n",
      "    Target (VI):    <start> h·∫øt   th√°ng ƒë·∫øn   nƒÉm , ch√∫ng t√¥i ki·ªÉm tra l·∫°i . <end>\n",
      "    Predicted (VI): cu·ªëi th√°ng nƒÉm , ch√∫ng t√¥i ki·ªÉm tra m·ªói nh√† h·ªçc h·ªèi .\n",
      "    BLEU score:     0.4090\n",
      "------------------------------------------------------------\n",
      "[8] Input (EN):     it  apos s very easy to spend money .\n",
      "    Target (VI):    <start> ti√™u ti·ªÅn th√¨ r·∫•t d·ªÖ . nh∆∞ng r·∫•t kh√≥ ƒë·ªÉ s·ª≠a <end>\n",
      "    Predicted (VI): n√≥ r·∫•t d·ªÖ d√†ng ti·ªÅn .\n",
      "    BLEU score:     0.0360\n",
      "------------------------------------------------------------\n",
      "[9] Input (EN):     it  apos s very difficult to improve the function of all those parts of the house , and for a whole house , the nine healthy living practices , we test , check and fix     items in every house .\n",
      "    Target (VI):    <start> ch·ª©c nƒÉng to√†n b·ªô v·∫≠t d·ª•ng trong nh√† . so to√†n b·ªô nh√† , c√πng   th·ª±c h√†nh s·ªëng kho·∫ª m·∫°nh , ch√∫ng t√¥i ki·ªÉm nghi·ªám v√† s·ª≠a     ƒë·ªì v·∫≠t trong m·ªói nh√† . <end>\n",
      "    Predicted (VI): r·∫•t kh√≥ ƒë·ªÉ c·∫£i thi·ªán ch·ª©c nƒÉng c·ªßa c√°c ph·∫ßn c·ªßa c√°c nh√† n√†y , nh√† c·ªßa nh√† , v√† cho c·∫£ nh√† , ng√¥i nh√† , v√† m·ªôt ng√¥i nh√† , v√† c·∫£ c√°c nh√† , m·ªôt cu·ªôc ng√¥i nh√† , v√† c√¥ng vi·ªác , ch√∫ng t√¥i ki·ªÉm tra , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm so√°t , v√† ki·ªÉm\n",
      "    BLEU score:     0.0039\n",
      "------------------------------------------------------------\n",
      "[10] Input (EN):     and these are the results we can get with our   ,     dollars .\n",
      "    Target (VI):    <start> v√† ƒë√¢y l√† k·∫øt qu·∫£ ch√∫ng t√¥i c√≥ t·ª´   .     ƒë√¥ la . <end>\n",
      "    Predicted (VI): v√† ƒë√¢y l√† nh·ªØng k·∫øt qu·∫£ m√† ch√∫ng ta c√≥ th·ªÉ l√†m v·ªõi ch√∫ng ta ,\n",
      "    BLEU score:     0.1074\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéØ ƒêi·ªÉm BLEU trung b√¨nh tr√™n t·∫≠p validation: 0.0689\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "#ƒê·ªãnh nghƒ©a l·∫°i m√¥ h√¨nh Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "#Kh·ªüi t·∫°o l·∫°i m√¥ h√¨nh\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "# T·∫°o m·ªôt input gi·∫£ ƒë·ªÉ build encoder/decoder\n",
    "sample_input = tf.random.uniform((1, 10), dtype=tf.int32, minval=0, maxval=vocab_inp_size)\n",
    "\n",
    "# Build encoder\n",
    "encoder.initialize_hidden_state(1)  # G·ªçi hidden\n",
    "_ = encoder(sample_input, encoder.initialize_hidden_state(1))\n",
    "\n",
    "# Build decoder\n",
    "sample_output = tf.random.uniform((1, 1), dtype=tf.int32, minval=0, maxval=vocab_tar_size)\n",
    "_ = decoder(sample_output, encoder.initialize_hidden_state(1), tf.random.uniform((1, 10, units)))\n",
    "\n",
    "\n",
    "\n",
    "#load tr·ªçng s·ªë\n",
    "encoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\encoder_epoch40.weights.h5')\n",
    "decoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\decoder_epoch40.weights.h5')\n",
    "\n",
    "# T√≠nh ƒë·ªô d√†i t·ªëi ƒëa cho ƒë·∫ßu v√†o v√† ƒë·∫ßu ra (n·∫øu ch∆∞a c√≥)\n",
    "max_length_inp = max(len(seq) for seq in en_tokenizer.texts_to_sequences(en_sentences))\n",
    "max_length_targ = max(len(seq) for seq in vi_tokenizer.texts_to_sequences(vi_sentences))\n",
    "\n",
    "\n",
    "# L·∫•y t·∫≠p validation t·ª´ 10% cu·ªëi\n",
    "val_input_sentences = en_sentences[-100:]\n",
    "val_target_sentences = vi_sentences[-100:]\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω 1 c√¢u\n",
    "def preprocess_sentence(sentence):\n",
    "    import numpy as np\n",
    "    if isinstance(sentence, np.ndarray):\n",
    "        if sentence.ndim == 0:\n",
    "            sentence = sentence.item()\n",
    "        else:\n",
    "            sentence = sentence[0]\n",
    "    elif isinstance(sentence, list):\n",
    "        if isinstance(sentence[0], str):\n",
    "            sentence = sentence[0]\n",
    "        else:\n",
    "            sentence = ' '.join(str(w) for w in sentence)\n",
    "    return sentence.lower().strip()\n",
    "\n",
    "# H√†m d·ªãch m·ªôt c√¢u\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# T√≠nh BLEU cho 1 c·∫∑p c√¢u\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p validation\n",
    "bleu_scores = []\n",
    "\n",
    "print(\"ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\\n\")\n",
    "for i in range(len(val_input_sentences)):\n",
    "    input_sentence = val_input_sentences[i]\n",
    "    target_sentence = val_target_sentences[i]\n",
    "\n",
    "    pred_sentence = evaluate(input_sentence)\n",
    "\n",
    "    ref = preprocess_sentence(target_sentence).split()\n",
    "    hyp = pred_sentence.split()\n",
    "\n",
    "    score = compute_bleu(ref, hyp)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "    # In 10 c√¢u ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra\n",
    "    if i < 10:\n",
    "        print(f\"[{i+1}] Input (EN):     {input_sentence}\")\n",
    "        print(f\"    Target (VI):    {target_sentence}\")\n",
    "        print(f\"    Predicted (VI): {pred_sentence}\")\n",
    "        print(f\"    BLEU score:     {score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# BLEU trung b√¨nh\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"\\n ƒêi·ªÉm BLEU trung b√¨nh tr√™n t·∫≠p validation: {average_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc964ef",
   "metadata": {},
   "source": [
    "5. Demo web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2628d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:26] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:40] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:47] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:27:56] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Jun/2025 02:28:02] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from flask import Flask, request, render_template_string\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# H√†m d·ªãch m·ªôt c√¢u\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Demo D·ªãch M√°y Anh-Vi·ªát</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background: #f4f6f8;\n",
    "            margin: 0;\n",
    "            padding: 40px;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            align-items: center;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        form {\n",
    "            background: #fff;\n",
    "            padding: 20px 30px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "            width: 100%;\n",
    "            max-width: 600px;\n",
    "        }\n",
    "        textarea {\n",
    "            width: 100%;\n",
    "            height: 100px;\n",
    "            padding: 10px;\n",
    "            font-size: 16px;\n",
    "            resize: none;\n",
    "            border: 1px solid #ccc;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        button {\n",
    "            margin-top: 10px;\n",
    "            padding: 10px 20px;\n",
    "            font-size: 16px;\n",
    "            background: #007bff;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        button:hover {\n",
    "            background: #0056b3;\n",
    "        }\n",
    "        .result {\n",
    "            margin-top: 30px;\n",
    "            background: #e8f0fe;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            max-width: 600px;\n",
    "            width: 100%;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>üåç D·ªãch M√°y Anh-Vi·ªát</h1>\n",
    "    <form method=\"POST\">\n",
    "        <textarea name=\"input_text\" placeholder=\"Nh·∫≠p c√¢u ti·∫øng Anh...\" required></textarea><br/>\n",
    "        <button type=\"submit\">D·ªãch</button>\n",
    "    </form>\n",
    "    {% if translation %}\n",
    "    <div class=\"result\">\n",
    "        <h2>K·∫øt qu·∫£ d·ªãch:</h2>\n",
    "        <p>{{ translation }}</p>\n",
    "    </div>\n",
    "    {% endif %}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    translation = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        input_text = request.form[\"input_text\"]\n",
    "        translation = evaluate(input_text)\n",
    "    return render_template_string(html, translation=translation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
