{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55f479e",
   "metadata": {},
   "source": [
    "1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76b8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªçc d·ªØ li·ªáu song ng·ªØ\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\data\\train.en', 'r', encoding='utf-8') as f:\n",
    "    en_sentences = f.read().strip().split('\\n')\n",
    "\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\data\\train.vi', 'r', encoding='utf-8') as f:\n",
    "    vi_sentences = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d738244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l√†m s·∫°ch\n",
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.lower().strip()               #lower chuy·ªÉn t·∫•t c·∫£ c√°c ch·ªØ th√†nh ch·ªØ th∆∞·ªùng, strip x√≥a kho·∫£ng tr·∫Øng ƒë·∫ßu v√† cu·ªëi c√¢u\n",
    "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s) #th√™m d·∫•u c√°ch tr∆∞·ªõc v√† sau c√°c d·∫•u c√¢u\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)       #lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng th·ª´a\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,‚Äô'`√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]\", \" \", s) #lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt\n",
    "    s = s.strip() \n",
    "    return s\n",
    "\n",
    "#l√†m s·∫°ch to√†n b·ªô c√°c c√¢u song ng·ªØ tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh\n",
    "en_sentences = [clean_sentence(s) for s in en_sentences]\n",
    "vi_sentences = [clean_sentence(s) for s in vi_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "938d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_sentences = ['<start> ' + s + ' <end>' for s in vi_sentences] #th√™m token start v√† end cho c√°c c√¢u ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eadd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o token cho c√°c c√¢u, chuy·ªÉn c√°c c√¢u th√†nh token ID\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Ti·∫øng Anh\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(en_sentences)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_sentences)\n",
    "\n",
    "#Ti·∫øng Vi·ªát\n",
    "vi_tokenizer = Tokenizer(filters='')\n",
    "vi_tokenizer.fit_on_texts(vi_sentences)\n",
    "vi_sequences = vi_tokenizer.texts_to_sequences(vi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8014ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#ƒë·ªô d√†i trong danh s√°ch ti·∫øng anh v√† ti·∫øng vi·ªát\n",
    "MAX_LEN = 80\n",
    "\n",
    "en_padded = pad_sequences(en_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "#√°p d·ª•ng padding cho t·∫•t c·∫£ chu·ªói senquences\n",
    "#Ch·ªâ d√πng 50.000 c√¢u ƒë·ªÉ th·ª≠\n",
    "en_sequences_small = en_sequences[:50000]\n",
    "vi_sequences_small = vi_sequences[:50000]\n",
    "\n",
    "en_padded = pad_sequences(en_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87674878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia ra hai ph·∫ßn train v√† valid (ƒë√°nh gi√°) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "en_train, en_val, vi_train, vi_val = train_test_split(en_padded, vi_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43fe91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o data set\n",
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(en_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((en_val, vi_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f2f2",
   "metadata": {},
   "source": [
    "2. X√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96eb80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªãnh nghƒ©a m√¥ h√¨nh Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91ad1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1   \n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f2318",
   "metadata": {},
   "source": [
    "3. Hu·∫•n luy·ªán m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c899156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒê·ªãnh nghƒ©a optimizer v√† loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90261354",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(en_padded)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_padded, vi_padded))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "701c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tf.constant(vi_tokenizer.word_index['<start>'], dtype=tf.int32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # T·∫°o dec_input b·∫±ng Tensor constant\n",
    "        dec_input = tf.expand_dims(tf.repeat(start_token, BATCH_SIZE), 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / tf.cast(targ.shape[1], tf.float32)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.2210\n",
      "Epoch 2 Loss 1.1623\n",
      "Epoch 3 Loss 1.1251\n",
      "Epoch 4 Loss 1.1146\n",
      "Epoch 5 Loss 1.0481\n",
      "Epoch 6 Loss 1.0573\n",
      "Epoch 7 Loss 1.0111\n",
      "Epoch 8 Loss 1.0157\n",
      "Epoch 9 Loss 0.9920\n",
      "Epoch 10 Loss 0.9568\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):                                                 #B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p\n",
    "    enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    \n",
    "# L∆∞u m√¥ h√¨nh m·ªói epoch\n",
    "encoder.save_weights(f'encoder_epoch{epoch+1}.weights.h5')\n",
    "decoder.save_weights(f'decoder_epoch{epoch+1}.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ab01a",
   "metadata": {},
   "source": [
    "4. ƒê√°nh gi√° hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c54add12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\n",
      "\n",
      "[1] Input (EN):     they apos re actually about improving houses .\n",
      "    Target (VI):    start m√† l√† c·∫£i thi·ªán nh√† c·ª≠a . end\n",
      "    Predicted (VI): h·ªç th·ª±c hi·ªán ra nh·ªØng nh√† thi·∫øt k·∫ø .\n",
      "    BLEU score:     0.0306\n",
      "------------------------------------------------------------\n",
      "[2] Input (EN):     we start on day one of every project we apos ve learned , we don apos t make promises , we don apos t do reports .\n",
      "    Target (VI):    start m·ªói ng√†y ch√∫ng t√¥i l√†m d·ª± √°n theo kinh nghi·ªám , ch√∫ng t√¥i kh√¥ng h·ª©a , kh√¥ng b√°o c√°o . end\n",
      "    Predicted (VI): ch√∫ng t√¥i b·∫Øt ƒë·∫ßu ƒëi ng√†y , m·ªôt nh√≥m ng∆∞·ªùi ta ƒë√£ h·ªçc ƒë∆∞·ª£c , ch√∫ng ta kh√¥ng th·ªÉ l√†m nh·ªØng √Ω t∆∞·ªüng , ch√∫ng ta kh√¥ng l√†m vi·ªác ƒë√†o t·∫°o .\n",
      "    BLEU score:     0.0343\n",
      "------------------------------------------------------------\n",
      "[3] Input (EN):     we arrive in the morning with tools , tons of equipment , trades , and we train up a local team on the first day to start work .\n",
      "    Target (VI):    start m·ªói s√°ng ch√∫ng t√¥i mang d·ª•ng c·ª• ƒë·∫øn , h√†ng t·∫•n thi·∫øt b·ªã , giao d·ªãch , v√† ƒë√†o t·∫°o c√°c nh√≥m b·∫£n ƒë·ªãa ƒë·ªÉ l√†m vi·ªác ngay ng√†y ƒë·∫ßu . end\n",
      "    Predicted (VI): ch√∫ng ta ƒë·∫øn trong m·ªôt v√†i tu·∫ßn , nh·ªØng c√¥ng c·ª• , h·ªç b·ªã thu h√∫t thu·ªëc , r·∫•t , , v√† ch√∫ng t√¥i ƒë√£ ƒëƒÉng k√≠ m·ªôt trong m·ªôt nh√≥m c·ªßa m·ªôt ng√†y ƒë·∫ßu ti√™n b·∫Øt ƒë·∫ßu l√†m .\n",
      "    BLEU score:     0.0338\n",
      "------------------------------------------------------------\n",
      "[4] Input (EN):     by the evening of the first day , a few houses in that community are better than when we started in the morning .\n",
      "    Target (VI):    start t·ªëi ng√†y ƒë·∫ßu ti√™n , m·ªôt s·ªë nh√† trong c·ªông ƒë·ªìng ƒë√≥ ƒë√£ t·ªët h∆°n so v·ªõi l√∫c s√°ng . end\n",
      "    Predicted (VI): khi b·ª©c ·∫£nh ng√†y ƒë·∫ßu ti√™n , m·ªôt v√†i nh√† nh√† trong c·ªông ƒë·ªìng h∆°n , khi ch√∫ng t√¥i b·∫Øt ƒë·∫ßu ·ªü ƒë√¢y .\n",
      "    BLEU score:     0.2594\n",
      "------------------------------------------------------------\n",
      "[5] Input (EN):     that work continues for six to months until all the houses are improved and we apos ve spent our budget of , dollars total per house .\n",
      "    Target (VI):    start c√¥ng vi·ªác ti·∫øp t·ª•c tri·ªÉn khai ƒë·∫øn th√°ng ƒë·∫øn khi m·ªçi nh√† ƒë·ªÅu ƒë∆∞·ª£c s·ª≠a v·ªõi chi ph√≠ trung b√¨nh . ƒë√¥ la cho m·ªói nh√† . end\n",
      "    Predicted (VI): ƒëi·ªÅu n√†y l√†m vi·ªác ti·∫øp t·ª•c v·ªõi c√°c th√°ng sau ƒë√≥ cho ƒë·∫øn khi c√°c nh√† m√°y m√≥c v√† ch√∫ng t√¥i ƒë√£ c·ªë g·∫Øng b·∫£o t√†ng c·ªßa ch√∫ng ta , v√† m·ªôt ng∆∞·ªùi ·ªü nh√† .\n",
      "    BLEU score:     0.0578\n",
      "------------------------------------------------------------\n",
      "[6] Input (EN):     that apos s our average budget .\n",
      "    Target (VI):    start ƒë√≥ l√† ng√¢n s√°ch trung b√¨nh . end\n",
      "    Predicted (VI): ƒë√≥ l√† ng∆∞·ªùi d√¢n ch·ªß .\n",
      "    BLEU score:     0.0652\n",
      "------------------------------------------------------------\n",
      "[7] Input (EN):     at the end of six months to a year , we test every house again .\n",
      "    Target (VI):    start h·∫øt th√°ng ƒë·∫øn nƒÉm , ch√∫ng t√¥i ki·ªÉm tra l·∫°i . end\n",
      "    Predicted (VI): cu·ªëi c√πng v·ªõi th√°ng , ch√∫ng t√¥i ƒë·ªçc m·ªôt nh√† .\n",
      "    BLEU score:     0.1100\n",
      "------------------------------------------------------------\n",
      "[8] Input (EN):     it apos s very easy to spend money .\n",
      "    Target (VI):    start ti√™u ti·ªÅn th√¨ r·∫•t d·ªÖ . nh∆∞ng r·∫•t kh√≥ ƒë·ªÉ s·ª≠a end\n",
      "    Predicted (VI): n√≥ r·∫•t d·ªÖ d√†ng ti·ªÅn .\n",
      "    BLEU score:     0.0360\n",
      "------------------------------------------------------------\n",
      "[9] Input (EN):     it apos s very difficult to improve the function of all those parts of the house , and for a whole house , the nine healthy living practices , we test , check and fix items in every house .\n",
      "    Target (VI):    start ch·ª©c nƒÉng to√†n b·ªô v·∫≠t d·ª•ng trong nh√† . so to√†n b·ªô nh√† , c√πng th·ª±c h√†nh s·ªëng kho·∫ª m·∫°nh , ch√∫ng t√¥i ki·ªÉm nghi·ªám v√† s·ª≠a ƒë·ªì v·∫≠t trong m·ªói nh√† . end\n",
      "    Predicted (VI): n√≥ r·∫•t kh√≥ ƒë·ªÉ c·∫£i thi·ªán nƒÉng l∆∞·ª£ng c·ªßa c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c b·∫°n trong c√°c\n",
      "    BLEU score:     0.0096\n",
      "------------------------------------------------------------\n",
      "[10] Input (EN):     and these are the results we can get with our , dollars .\n",
      "    Target (VI):    start v√† ƒë√¢y l√† k·∫øt qu·∫£ ch√∫ng t√¥i c√≥ t·ª´ . ƒë√¥ la . end\n",
      "    Predicted (VI): v√† nh·ªØng t√°c ph·∫©m m√† ch√∫ng ta c√≥ th·ªÉ c√≥ th·ªÉ c√≥ th·ªÉ v·ªõi nhau ,\n",
      "    BLEU score:     0.0207\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéØ ƒêi·ªÉm BLEU trung b√¨nh tr√™n t·∫≠p validation: 0.0722\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "encoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\encoder_epoch10.weights.h5')\n",
    "decoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\\decoder_epoch10.weights.h5')\n",
    "\n",
    "# L·∫•y t·∫≠p validation t·ª´ 10% cu·ªëi\n",
    "val_input_sentences = en_sentences[-100:]\n",
    "val_target_sentences = vi_sentences[-100:]\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω 1 c√¢u\n",
    "def preprocess_sentence(sentence):\n",
    "    import numpy as np\n",
    "    if isinstance(sentence, np.ndarray):\n",
    "        if sentence.ndim == 0:\n",
    "            sentence = sentence.item()\n",
    "        else:\n",
    "            sentence = sentence[0]\n",
    "    elif isinstance(sentence, list):\n",
    "        if isinstance(sentence[0], str):\n",
    "            sentence = sentence[0]\n",
    "        else:\n",
    "            sentence = ' '.join(str(w) for w in sentence)\n",
    "    return sentence.lower().strip()\n",
    "\n",
    "# H√†m d·ªãch m·ªôt c√¢u\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# T√≠nh BLEU cho 1 c·∫∑p c√¢u\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p validation\n",
    "bleu_scores = []\n",
    "\n",
    "print(\"ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\\n\")\n",
    "for i in range(len(val_input_sentences)):\n",
    "    input_sentence = val_input_sentences[i]\n",
    "    target_sentence = val_target_sentences[i]\n",
    "\n",
    "    pred_sentence = evaluate(input_sentence)\n",
    "\n",
    "    ref = preprocess_sentence(target_sentence).split()\n",
    "    hyp = pred_sentence.split()\n",
    "\n",
    "    score = compute_bleu(ref, hyp)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "    # In 10 c√¢u ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra\n",
    "    if i < 10:\n",
    "        print(f\"[{i+1}] Input (EN):     {input_sentence}\")\n",
    "        print(f\"    Target (VI):    {target_sentence}\")\n",
    "        print(f\"    Predicted (VI): {pred_sentence}\")\n",
    "        print(f\"    BLEU score:     {score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# BLEU trung b√¨nh\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"\\nüéØ ƒêi·ªÉm BLEU trung b√¨nh tr√™n t·∫≠p validation: {average_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc964ef",
   "metadata": {},
   "source": [
    "5. Demo web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2628d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:22] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:28] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:56] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:13:58] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:05] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:15] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:18] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from flask import Flask, request, render_template_string\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# H√†m d·ªãch m·ªôt c√¢u\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head><meta charset=\"UTF-8\"><title>Demo D·ªãch M√°y</title></head>\n",
    "<body>\n",
    "<h1>Demo D·ªãch M√°y Anh-Vi·ªát</h1>\n",
    "<form method=\"POST\">\n",
    "    <textarea name=\"input_text\" rows=\"3\" cols=\"50\" required></textarea><br/>\n",
    "    <button type=\"submit\">D·ªãch</button>\n",
    "</form>\n",
    "{% if translation %}\n",
    "<h2>K·∫øt qu·∫£ d·ªãch:</h2>\n",
    "<p>{{ translation }}</p>\n",
    "{% endif %}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    translation = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        input_text = request.form[\"input_text\"]\n",
    "        translation = evaluate(input_text)\n",
    "    return render_template_string(html, translation=translation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
