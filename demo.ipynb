{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55f479e",
   "metadata": {},
   "source": [
    "1. Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76b8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc dữ liệu song ngữ\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\data\\train.en', 'r', encoding='utf-8') as f:\n",
    "    en_sentences = f.read().strip().split('\\n')\n",
    "\n",
    "with open(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\data\\train.vi', 'r', encoding='utf-8') as f:\n",
    "    vi_sentences = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d738244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#làm sạch\n",
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.lower().strip()               #lower chuyển tất cả các chữ thành chữ thường, strip xóa khoảng trắng đầu và cuối câu\n",
    "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s) #thêm dấu cách trước và sau các dấu câu\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)       #loại bỏ các khoảng trắng thừa\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,’'`àáảãạăằắẳẵặâầấẩẫậèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđ]\", \" \", s) #loại bỏ các ký tự không cần thiết\n",
    "    s = s.strip() \n",
    "    return s\n",
    "\n",
    "#làm sạch toàn bộ các câu song ngữ trước khi đưa vào mô hình\n",
    "en_sentences = [clean_sentence(s) for s in en_sentences]\n",
    "vi_sentences = [clean_sentence(s) for s in vi_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "938d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_sentences = ['<start> ' + s + ' <end>' for s in vi_sentences] #thêm token start và end cho các câu tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eadd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo token cho các câu, chuyển các câu thành token ID\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Tiếng Anh\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(en_sentences)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_sentences)\n",
    "\n",
    "#Tiếng Việt\n",
    "vi_tokenizer = Tokenizer(filters='')\n",
    "vi_tokenizer.fit_on_texts(vi_sentences)\n",
    "vi_sequences = vi_tokenizer.texts_to_sequences(vi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8014ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#độ dài trong danh sách tiếng anh và tiếng việt\n",
    "MAX_LEN = 80\n",
    "\n",
    "en_padded = pad_sequences(en_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "#áp dụng padding cho tất cả chuỗi senquences\n",
    "#Chỉ dùng 50.000 câu để thử\n",
    "en_sequences_small = en_sequences[:50000]\n",
    "vi_sequences_small = vi_sequences[:50000]\n",
    "\n",
    "en_padded = pad_sequences(en_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "vi_padded = pad_sequences(vi_sequences_small, maxlen=MAX_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87674878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia ra hai phần train và valid (đánh giá) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "en_train, en_val, vi_train, vi_val = train_test_split(en_padded, vi_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43fe91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo data set\n",
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(en_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((en_val, vi_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f2f2",
   "metadata": {},
   "source": [
    "2. Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96eb80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Định nghĩa mô hình Encoder, Attention, Decoder\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Layer\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden size)\n",
    "        # values shape: (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91ad1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo mô hình\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(en_tokenizer.word_index) + 1   \n",
    "vocab_tar_size = len(vi_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f2318",
   "metadata": {},
   "source": [
    "3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c899156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Định nghĩa optimizer và loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90261354",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(en_padded)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_padded, vi_padded))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "701c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tf.constant(vi_tokenizer.word_index['<start>'], dtype=tf.int32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # Tạo dec_input bằng Tensor constant\n",
    "        dec_input = tf.expand_dims(tf.repeat(start_token, BATCH_SIZE), 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / tf.cast(targ.shape[1], tf.float32)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.2210\n",
      "Epoch 2 Loss 1.1623\n",
      "Epoch 3 Loss 1.1251\n",
      "Epoch 4 Loss 1.1146\n",
      "Epoch 5 Loss 1.0481\n",
      "Epoch 6 Loss 1.0573\n",
      "Epoch 7 Loss 1.0111\n",
      "Epoch 8 Loss 1.0157\n",
      "Epoch 9 Loss 0.9920\n",
      "Epoch 10 Loss 0.9568\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):                                                 #Bắt đầu vòng lặp\n",
    "    enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    \n",
    "# Lưu mô hình mỗi epoch\n",
    "encoder.save_weights(f'encoder_epoch{epoch+1}.weights.h5')\n",
    "decoder.save_weights(f'decoder_epoch{epoch+1}.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ab01a",
   "metadata": {},
   "source": [
    "4. Đánh giá huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c54add12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đánh giá mô hình trên tập validation...\n",
      "\n",
      "[1] Input (EN):     they apos re actually about improving houses .\n",
      "    Target (VI):    start mà là cải thiện nhà cửa . end\n",
      "    Predicted (VI): họ thực hiện ra những nhà thiết kế .\n",
      "    BLEU score:     0.0306\n",
      "------------------------------------------------------------\n",
      "[2] Input (EN):     we start on day one of every project we apos ve learned , we don apos t make promises , we don apos t do reports .\n",
      "    Target (VI):    start mỗi ngày chúng tôi làm dự án theo kinh nghiệm , chúng tôi không hứa , không báo cáo . end\n",
      "    Predicted (VI): chúng tôi bắt đầu đi ngày , một nhóm người ta đã học được , chúng ta không thể làm những ý tưởng , chúng ta không làm việc đào tạo .\n",
      "    BLEU score:     0.0343\n",
      "------------------------------------------------------------\n",
      "[3] Input (EN):     we arrive in the morning with tools , tons of equipment , trades , and we train up a local team on the first day to start work .\n",
      "    Target (VI):    start mỗi sáng chúng tôi mang dụng cụ đến , hàng tấn thiết bị , giao dịch , và đào tạo các nhóm bản địa để làm việc ngay ngày đầu . end\n",
      "    Predicted (VI): chúng ta đến trong một vài tuần , những công cụ , họ bị thu hút thuốc , rất , , và chúng tôi đã đăng kí một trong một nhóm của một ngày đầu tiên bắt đầu làm .\n",
      "    BLEU score:     0.0338\n",
      "------------------------------------------------------------\n",
      "[4] Input (EN):     by the evening of the first day , a few houses in that community are better than when we started in the morning .\n",
      "    Target (VI):    start tối ngày đầu tiên , một số nhà trong cộng đồng đó đã tốt hơn so với lúc sáng . end\n",
      "    Predicted (VI): khi bức ảnh ngày đầu tiên , một vài nhà nhà trong cộng đồng hơn , khi chúng tôi bắt đầu ở đây .\n",
      "    BLEU score:     0.2594\n",
      "------------------------------------------------------------\n",
      "[5] Input (EN):     that work continues for six to months until all the houses are improved and we apos ve spent our budget of , dollars total per house .\n",
      "    Target (VI):    start công việc tiếp tục triển khai đến tháng đến khi mọi nhà đều được sửa với chi phí trung bình . đô la cho mỗi nhà . end\n",
      "    Predicted (VI): điều này làm việc tiếp tục với các tháng sau đó cho đến khi các nhà máy móc và chúng tôi đã cố gắng bảo tàng của chúng ta , và một người ở nhà .\n",
      "    BLEU score:     0.0578\n",
      "------------------------------------------------------------\n",
      "[6] Input (EN):     that apos s our average budget .\n",
      "    Target (VI):    start đó là ngân sách trung bình . end\n",
      "    Predicted (VI): đó là người dân chủ .\n",
      "    BLEU score:     0.0652\n",
      "------------------------------------------------------------\n",
      "[7] Input (EN):     at the end of six months to a year , we test every house again .\n",
      "    Target (VI):    start hết tháng đến năm , chúng tôi kiểm tra lại . end\n",
      "    Predicted (VI): cuối cùng với tháng , chúng tôi đọc một nhà .\n",
      "    BLEU score:     0.1100\n",
      "------------------------------------------------------------\n",
      "[8] Input (EN):     it apos s very easy to spend money .\n",
      "    Target (VI):    start tiêu tiền thì rất dễ . nhưng rất khó để sửa end\n",
      "    Predicted (VI): nó rất dễ dàng tiền .\n",
      "    BLEU score:     0.0360\n",
      "------------------------------------------------------------\n",
      "[9] Input (EN):     it apos s very difficult to improve the function of all those parts of the house , and for a whole house , the nine healthy living practices , we test , check and fix items in every house .\n",
      "    Target (VI):    start chức năng toàn bộ vật dụng trong nhà . so toàn bộ nhà , cùng thực hành sống khoẻ mạnh , chúng tôi kiểm nghiệm và sửa đồ vật trong mỗi nhà . end\n",
      "    Predicted (VI): nó rất khó để cải thiện năng lượng của các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các bạn trong các\n",
      "    BLEU score:     0.0096\n",
      "------------------------------------------------------------\n",
      "[10] Input (EN):     and these are the results we can get with our , dollars .\n",
      "    Target (VI):    start và đây là kết quả chúng tôi có từ . đô la . end\n",
      "    Predicted (VI): và những tác phẩm mà chúng ta có thể có thể có thể với nhau ,\n",
      "    BLEU score:     0.0207\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎯 Điểm BLEU trung bình trên tập validation: 0.0722\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "encoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\encoder_epoch10.weights.h5')\n",
    "decoder.load_weights(r'C:\\Users\\phamq\\Downloads\\BTL - Xử lý ngôn ngữ tự nhiên\\decoder_epoch10.weights.h5')\n",
    "\n",
    "# Lấy tập validation từ 10% cuối\n",
    "val_input_sentences = en_sentences[-100:]\n",
    "val_target_sentences = vi_sentences[-100:]\n",
    "\n",
    "# Tiền xử lý 1 câu\n",
    "def preprocess_sentence(sentence):\n",
    "    import numpy as np\n",
    "    if isinstance(sentence, np.ndarray):\n",
    "        if sentence.ndim == 0:\n",
    "            sentence = sentence.item()\n",
    "        else:\n",
    "            sentence = sentence[0]\n",
    "    elif isinstance(sentence, list):\n",
    "        if isinstance(sentence[0], str):\n",
    "            sentence = sentence[0]\n",
    "        else:\n",
    "            sentence = ' '.join(str(w) for w in sentence)\n",
    "    return sentence.lower().strip()\n",
    "\n",
    "# Hàm dịch một câu\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# Tính BLEU cho 1 cặp câu\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "# Đánh giá trên tập validation\n",
    "bleu_scores = []\n",
    "\n",
    "print(\"Đang đánh giá mô hình trên tập validation...\\n\")\n",
    "for i in range(len(val_input_sentences)):\n",
    "    input_sentence = val_input_sentences[i]\n",
    "    target_sentence = val_target_sentences[i]\n",
    "\n",
    "    pred_sentence = evaluate(input_sentence)\n",
    "\n",
    "    ref = preprocess_sentence(target_sentence).split()\n",
    "    hyp = pred_sentence.split()\n",
    "\n",
    "    score = compute_bleu(ref, hyp)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "    # In 10 câu đầu để kiểm tra\n",
    "    if i < 10:\n",
    "        print(f\"[{i+1}] Input (EN):     {input_sentence}\")\n",
    "        print(f\"    Target (VI):    {target_sentence}\")\n",
    "        print(f\"    Predicted (VI): {pred_sentence}\")\n",
    "        print(f\"    BLEU score:     {score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# BLEU trung bình\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"\\n🎯 Điểm BLEU trung bình trên tập validation: {average_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc964ef",
   "metadata": {},
   "source": [
    "5. Demo web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2628d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:22] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:28] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:12:56] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:13:58] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:05] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:15] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2025 07:14:18] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from flask import Flask, request, render_template_string\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Hàm dịch một câu\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = en_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden = encoder.initialize_hidden_state(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([vi_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_word = vi_tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result += predicted_word + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head><meta charset=\"UTF-8\"><title>Demo Dịch Máy</title></head>\n",
    "<body>\n",
    "<h1>Demo Dịch Máy Anh-Việt</h1>\n",
    "<form method=\"POST\">\n",
    "    <textarea name=\"input_text\" rows=\"3\" cols=\"50\" required></textarea><br/>\n",
    "    <button type=\"submit\">Dịch</button>\n",
    "</form>\n",
    "{% if translation %}\n",
    "<h2>Kết quả dịch:</h2>\n",
    "<p>{{ translation }}</p>\n",
    "{% endif %}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    translation = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        input_text = request.form[\"input_text\"]\n",
    "        translation = evaluate(input_text)\n",
    "    return render_template_string(html, translation=translation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
